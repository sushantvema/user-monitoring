




# User Monitoring Pipeline Design Document


# Abstract

The goal of the User Monitoring team is to generate and update user credibility scores to monitor the quality of tasks responses for users contributing to Public Editor Tagworks tasks.


# **Definitions**

**User Credibility Scores (UCS)**: scores between 0 and 1 that are updated based on how accurate a user’s task responses are compared to some consensus answer

**Consensus answer**: the “correct” task response that each user’s answer will be compared to for updating user credibility scores. This will either be generated by finding the consensus answer between all users on that task (IAA algorithm) or the task responses from some gold-standard users (ex. Nick, Emlen, Eric)

**Datahunt file**: files that contain the data about task responses for a given task

**Task score**: the user’s average score on questions for a given task


# **Pipeline Requirements**



 

 - Datahunt Tracker: JSON file / Python dictionary / two-column table somewhere that contains the processed datahunt names/IDs along with the number of rows that have been processed in that datahunt 
	 - Two column table in MySQL database
	 - **[Step 1]** For each iteration of running the pipeline, we will get passed in a datahunt of a certain task which has an id X. Sort the datahunt file by timestamp and process all of those rows in the pipeline (generate credibility scores)
		 -  Let’s say the datahunt is updated sometime in the future (i.e more users completed this task). To address this and avoid double counting, we store information about how many rows of this given datahunt X we have processed into the two column table as a key-value pair
			 - { datahunt_id : len(datahunt_old) }
		 - Now to process the updated datahunt file, we have to remove the first len(datahunt_old) number of rows from the updated datahunt file sorted in chronological order. Then we process them as usual.
 -  Datahunt files, along with the corresponding IAA outputs and Gold standard outputs
 - User Credibility Score Table: a table containing the UUIDs of all users who have ever completed a task, along with their current UCS
	 - Two column table in MySQL database, with user_id and current UCS of that user
	 - Updated every time the pipeline is finished running
 - Previous Task Scores Table: a table containing the task scores of all tasks completed by all users, along with timestamps
	 -  Four column table in MySQL database, with timestamp, quiz_task_uuid, user_uuid, task_score
	 - Potential future issues: garbage cleaning when table gets too big


# **Pipeline Overview**

Suppose this pipeline runs each time there is a new datahunt in which users should be scored.


### Step 1: Preprocessing

Remove the rows that have already been processed in this datahunt (based on the name/ID of the datahunt), and pass the remaining rows to the rest of the pipeline. Because the Datahunt Tracker will keep track of the _n_ = number of rows that have been processed for this datahunt, all we need to do is sort the new datahunt by timestamp, and remove the first _n_ rows.


### Step 2: Identifying the Consensus Answers

The consensus answers may either come from the IAA algorithm or some gold-standard user. Gold-standard answers take priority over IAA algorithm answers, so the consensus answer bank for questions in this task will be created by taking the gold-standard answer if available, otherwise taking the IAA algorithm-determined consensus answer.


### Step 3: Scoring Users

There are three types of questions. The word “correct” will be used to refer to selecting the same answer as the determined consensus answer.



1. “Select one” nominal questions
     For these nominal questions, users will be scored 1 if they answer correctly, 0 otherwise.
2. “Select one” ordinal questions
     For these ordinal questions, users will be penalized based on how far they are from the correct answer. Suppose there are _n_ possible nominal responses, the user will receive a score of _1 - ((selected answer - consensus answer) / n)_. If the user selects the consensus answer, they will be scored 1. If the consensus answer is the “maximum” answer by the nominal ordering and the user selects the “minimum” answer by the same ordering, they will receive a score of 0. The flipped situation will have the same result. All other situations will be between 0 and 1.
3. “Select all that apply” questions
    How this type of question is scored depends on how much we want to penalize false positives over false negatives. For now, we’ll assume they hold the same weight – then this type of question will be scored by taking the proportion of answer selections made correctly (either selected or not selected).

For now, all questions will be weighted equally when generating a user’s task score. This scoring method will take into account highlights for comparison with the consensus highlight for each question/answer, but that will come in a future version.


### Step 4: Adding this Task Score to the Previous Task Scores Database

Self-explanatory, we want to add a row to the Previous Task Scores Database with columns contributor UUID, timestamp, and task score for each user that completed a task in this datahunt.


### Step 5: Updating the User Credibility Score

The following formula represents the process for updating any given user’s User Credibility Score:

_**UCS** = old_UCS * (1 - c) + task_score * c_

_**c** = logistic(var_scores / (np.log(num_task_scores + 1) / (np.log(a))), 10, 0.2), c is in (0,1)_

_**a** = 1000_

_**num_task_scores** = total number of tasks done_

_**var_scores** = variance of the last n scores, n = min(10, int(np.sqrt(num_task_scores)) + 1)_


       def logistic(x, k, offset):
	       return 1 / (1 + np.e**(-k * (x - offset)))

The reasoning behind this general formula is as follows: given any user with a user reputation score, we want to be able to update the user reputation score each time the user completes a task. We do this by having the new UCS being a convex combination of the previous UCS and the task score of the newly completed task. Thus, the most recent task score will account for a proportion, _c_, of the new UCS.

This _c_ value is between 0 and 1, and is dynamic depending on the user and their past trends in completing tasks.

A logistic function is used to ensure that _c_ stays within 0 and 1 as this range allows the UCS update function to remain valid. It is shown below, where k = 10 is the growth factor that determines the steepness of the curve - increasing the responsiveness to new task scores - and o is the offset, bringing the range between 0.1 and 1 which centers _c_ around typical values and still allows it to be fairly high and low.


**![](https://lh6.googleusercontent.com/u9b9WgaJD5eTYhjFdMZRGz97e2RQrlxdXW5o_lBvi8f0aO2fGPPo064i00ikDvnkQseukSMQu8-Uew03w2t1P0HdIyBZd2qHp6_escqzmtw3FPG8OpmkzXOLvrl5dmtbuR6TB4DK)**
**![](https://lh6.googleusercontent.com/pEIBfmTwH25VCEqwEhzrieYfYHxvXgbnZRNlGfmyO7YseJV6_p969iJE4T_T12jIDEMOJIWWq_qqFhg4EiiRqKI-M5xdIfNa7IZLKJMWT7MFEl-506YAhl8LnZStGdF1kv9ZuA7g)**

The input to the logistic function, _x_, is the dynamic formula that depends on the variance of the last n task scores and on the total number of tasks done so far. Specifically, it is the variance of the last n scores divided by the log base a of the total number of tasks done, where the base a is a constant 1000. Dividing _np.log(num_task_scores + 1)_ by _np.log(a)_ in the code is just applying the change of base formula for logs since _np.log()_ is base 10 by default. This value was experimentally found and makes the function more responsive. Finally, n is computed to be the square root of the total number of tasks done, capped out at 10. Letting _n_ be maximum 10 is reasonable since it corresponds to users completing 100 tasks which is in reality quite a bit.

The User Credibility Scores will be updated for each user in the current datahunt, by pulling the _n_ most recent task scores of each user from the Previous Task Scores Database, and writing the updated User Credibility Scores into the User Credibility Scores Database.


### Step 6: Updating the Datahunt Tracker

After all task responses in the datahunt have been scored and the corresponding users have their UCSs updated, we will update the Datahunt Tracker to reflect that all the rows in this datahunt have been processed. This will be done by updating the row in the Datahunt Tracker corresponding to this datahunt name/ID to the number of rows/length of the datahunt just processed.

